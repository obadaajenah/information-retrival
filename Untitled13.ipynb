{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a080995-51fb-402c-b761-3eee98995236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, delimiter='\\t', header=None, names=['pid', 'text'])\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"Error reading the dataset file: {e}\")\n",
    "        sys.exit(1)\n",
    "    return data\n",
    "\n",
    "def read_cleaned_texts(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r' , encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            return [line.strip() for line in lines]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {file_path} does not exist.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "def save_vectors_to_binary_file(vector, filename):\n",
    "        \n",
    "        with open(filename, \"wb\") as file:\n",
    "          pickle.dump(vector, file)\n",
    "\n",
    "def convert_csv_to_tsv(input_csv_path, output_tsv_path):\n",
    "    try:\n",
    "        # قراءة ملف CSV\n",
    "        df = pd.read_csv(input_csv_path)\n",
    "        \n",
    "        # حفظ البيانات في ملف TSV\n",
    "        df.to_csv(output_tsv_path, sep='\\t', index=False)\n",
    "        print(f\"تم تحويل الملف بنجاح وحفظه في {output_tsv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"حدث خطأ أثناء تحويل الملف: {e}\")\n",
    "print(\"1\")\n",
    "\n",
    "\n",
    "def load_dataset2(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, delimiter='\\t')\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"Error reading the dataset file: {e}\")\n",
    "        sys.exit(1)\n",
    "    return data\n",
    "\n",
    "def read_csv_to_array(file_path):\n",
    "    # قراءة ملف CSV باستخدام pandas\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # تحويل إطار البيانات إلى مصفوفة من القوائم\n",
    "    data_array = df.values.tolist()\n",
    "    \n",
    "    return data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b69ba4d-a346-443c-aaad-0926c5fa9d4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastapi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastAPI, Request\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01muvicorn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fastapi'"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, Request\n",
    "import uvicorn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from fastapi import FastAPI, Request\n",
    "import uvicorn\n",
    "print(\"2\")\n",
    "# كلاس لتحويل النصوص إلى متجهات\n",
    "class TextIndexer:\n",
    " \n",
    "\n",
    " def clean_space(column1 , column2,column3,column4):\n",
    "        column1 = column1.fillna('')\n",
    "        column2 = column2.fillna('')\n",
    "        column3= column3.fillna('')\n",
    "        column4 = column4.fillna('')\n",
    "        return column1,column2 ,column3 ,column4\n",
    "\n",
    "\n",
    "# إعداد FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "# إنشاء كائن من TextIndexer\n",
    "indexer = TextIndexer()\n",
    "###################    The first dataset ############################33333333\n",
    "\n",
    "@app.post(\"/index_texts\")\n",
    "async def index_texts(request: Request):\n",
    "    try:\n",
    "        print(\"start\")\n",
    "        # استخراج البيانات من الطلب\n",
    "        data = await request.json()\n",
    "        \n",
    "        data_file = data.get(\"cleanned_data_file\")\n",
    "\n",
    "        data_texts = read_cleaned_texts(data_file)\n",
    "         \n",
    "        if  data_texts is None :\n",
    "            return {\"error\": \" 'data' is fields are required.\"}\n",
    "       \n",
    "        vectorizer = TfidfVectorizer()\n",
    "        # تحويل النصوص إلى متجهات\n",
    "        data_vectors = vectorizer.fit_transform( data_texts)\n",
    "       \n",
    "        data_vector_file=r\"E:\\DocumentVector.pkl\"\n",
    "        vectorizer_file= r\"E:\\TfidfVector.pkl\"\n",
    "        save_vectors_to_binary_file(data_vectors ,data_vector_file)\n",
    "        \n",
    "        save_vectors_to_binary_file(vectorizer ,vectorizer_file)\n",
    "        \n",
    "        return {\n",
    "            \"data_vectors_file\": data_vector_file,\n",
    "            \"vectorizer_file\": vectorizer_file,\n",
    "            \n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "################# The secound dataset ########################\n",
    "\n",
    "@app.post(\"/index_texts_cli\")\n",
    "async def index_texts(request: Request):\n",
    "    try:\n",
    "       \n",
    "        print(\"start\")\n",
    "        # استخراج البيانات من الطلب\n",
    "        data = await request.json()\n",
    "    \n",
    "        # print(data)\n",
    "        column1 = data.get(\"column1\")\n",
    "        column2 = data.get(\"column2\")\n",
    "        column3 = data.get(\"column3\")\n",
    "        column4 = data.get(\"column4\")\n",
    "        \n",
    "        vectorizer = TfidfVectorizer()\n",
    "       \n",
    "        column1_vec = vectorizer.fit_transform(column1)\n",
    "        column2_vec = vectorizer.fit_transform(column2)\n",
    "        column3_vec = vectorizer.fit_transform(column3)\n",
    "        column4_vec = vectorizer.fit_transform(column4)\n",
    "\n",
    "        combined_vocabulary = set(vectorizer.vocabulary_.keys())\n",
    "      \n",
    "        # إعادة بناء vectorizer بناءً على المفردات المدمجة\n",
    "        combined_vocabulary = list(combined_vocabulary)\n",
    "        combined_vectorizer = TfidfVectorizer(vocabulary=combined_vocabulary)\n",
    "    \n",
    "        # إعادة تحويل النصوص باستخدام الـ vectorizer المدمج\n",
    "        column1_vec = combined_vectorizer.fit_transform(column1)\n",
    "        column2_vec = combined_vectorizer.fit_transform(column2)\n",
    "        column3_vec = combined_vectorizer.fit_transform(column3)\n",
    "        column4_vec = combined_vectorizer.fit_transform(column4)\n",
    "        \n",
    "        col1_weight = 4.0  \n",
    "        col4_weight = 1.5\n",
    "        # حساب المتوسط المرجح\n",
    "        weighted_col1_tfidf = col1_weight * column1_vec\n",
    "       \n",
    "        weighted_col4_tfidf = col4_weight * column4_vec\n",
    "        # جمع المصفوفات\n",
    "        sum_tfidf = weighted_col1_tfidf + column2_vec + weighted_col4_tfidf + column4_vec\n",
    "\n",
    "        # حساب المتوسط المرجح\n",
    "        average_tfidf = sum_tfidf / (col1_weight + 1 + col4_weight + 1)\n",
    "       \n",
    "              \n",
    "        data_vector_file=r\"E:\\DocumentVector_clinical.pkl\"\n",
    "        vectorizer_file= r\"E:\\TfidfVector_clinical.pkl\"\n",
    "\n",
    "        save_vectors_to_binary_file(average_tfidf ,data_vector_file)\n",
    "       \n",
    "        save_vectors_to_binary_file(combined_vectorizer ,vectorizer_file)\n",
    "        \n",
    "        return {\n",
    "            \"data_vectors_file\": data_vector_file,\n",
    "            \"vectorizer_file\": vectorizer_file,\n",
    "            \n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "177ca31a-e174-45d2-9270-b57deaf072eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi\n",
      "  Using cached fastapi-0.111.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi)\n",
      "  Using cached starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in d:\\annkonda2\\lib\\site-packages (from fastapi) (1.10.12)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\annkonda2\\lib\\site-packages (from fastapi) (4.9.0)\n",
      "Collecting fastapi-cli>=0.0.2 (from fastapi)\n",
      "  Using cached fastapi_cli-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting httpx>=0.23.0 (from fastapi)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.2 in d:\\annkonda2\\lib\\site-packages (from fastapi) (3.1.3)\n",
      "Collecting python-multipart>=0.0.7 (from fastapi)\n",
      "  Using cached python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in d:\\annkonda2\\lib\\site-packages (from fastapi) (5.4.0)\n",
      "Collecting orjson>=3.2.1 (from fastapi)\n",
      "  Downloading orjson-3.10.3-cp311-none-win_amd64.whl.metadata (50 kB)\n",
      "     ---------------------------------------- 0.0/50.9 kB ? eta -:--:--\n",
      "     -------- ------------------------------- 10.2/50.9 kB ? eta -:--:--\n",
      "     -------- ------------------------------- 10.2/50.9 kB ? eta -:--:--\n",
      "     -------- ------------------------------- 10.2/50.9 kB ? eta -:--:--\n",
      "     -------- ------------------------------- 10.2/50.9 kB ? eta -:--:--\n",
      "     --------------- ----------------------- 20.5/50.9 kB 54.8 kB/s eta 0:00:01\n",
      "     ----------------------- --------------- 30.7/50.9 kB 87.5 kB/s eta 0:00:01\n",
      "     ----------------------- --------------- 30.7/50.9 kB 87.5 kB/s eta 0:00:01\n",
      "     ----------------------- --------------- 30.7/50.9 kB 87.5 kB/s eta 0:00:01\n",
      "     ----------------------- --------------- 30.7/50.9 kB 87.5 kB/s eta 0:00:01\n",
      "     ----------------------- --------------- 30.7/50.9 kB 87.5 kB/s eta 0:00:01\n",
      "     ----------------------- --------------- 30.7/50.9 kB 87.5 kB/s eta 0:00:01\n",
      "     ----------------------- --------------- 30.7/50.9 kB 87.5 kB/s eta 0:00:01\n",
      "     ----------------------- --------------- 30.7/50.9 kB 87.5 kB/s eta 0:00:01\n",
      "     ----------------------- --------------- 30.7/50.9 kB 87.5 kB/s eta 0:00:01\n",
      "     ----------------------- --------------- 30.7/50.9 kB 87.5 kB/s eta 0:00:01\n",
      "     ----------------------- --------------- 30.7/50.9 kB 87.5 kB/s eta 0:00:01\n",
      "     ------------------------------- ------- 41.0/50.9 kB 41.9 kB/s eta 0:00:01\n",
      "     --------------------------------------- 50.9/50.9 kB 48.3 kB/s eta 0:00:00\n",
      "Collecting email_validator>=2.0.0 (from fastapi)\n",
      "  Using cached email_validator-2.1.1-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0->fastapi)\n",
      "  Downloading uvicorn-0.30.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi)\n",
      "  Using cached dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: idna>=2.0.0 in d:\\annkonda2\\lib\\site-packages (from email_validator>=2.0.0->fastapi) (3.4)\n",
      "Collecting typer>=0.12.3 (from fastapi-cli>=0.0.2->fastapi)\n",
      "  Using cached typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: anyio in d:\\annkonda2\\lib\\site-packages (from httpx>=0.23.0->fastapi) (4.2.0)\n",
      "Requirement already satisfied: certifi in d:\\annkonda2\\lib\\site-packages (from httpx>=0.23.0->fastapi) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx>=0.23.0->fastapi)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: sniffio in d:\\annkonda2\\lib\\site-packages (from httpx>=0.23.0->fastapi) (1.3.0)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.23.0->fastapi)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\annkonda2\\lib\\site-packages (from jinja2>=2.11.2->fastapi) (2.1.3)\n",
      "Requirement already satisfied: click>=7.0 in d:\\annkonda2\\lib\\site-packages (from uvicorn>=0.12.0->uvicorn[standard]>=0.12.0->fastapi) (8.1.7)\n",
      "Requirement already satisfied: colorama>=0.4 in d:\\annkonda2\\lib\\site-packages (from uvicorn[standard]>=0.12.0->fastapi) (0.4.6)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.12.0->fastapi)\n",
      "  Downloading httptools-0.6.1-cp311-cp311-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in d:\\annkonda2\\lib\\site-packages (from uvicorn[standard]>=0.12.0->fastapi) (0.21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\annkonda2\\lib\\site-packages (from uvicorn[standard]>=0.12.0->fastapi) (6.0.1)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0->fastapi)\n",
      "  Downloading watchfiles-0.22.0-cp311-none-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.12.0->fastapi)\n",
      "  Downloading websockets-12.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\annkonda2\\lib\\site-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (13.3.5)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in d:\\annkonda2\\lib\\site-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\annkonda2\\lib\\site-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\annkonda2\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (0.1.0)\n",
      "Using cached fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
      "Using cached email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
      "Using cached fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Downloading orjson-3.10.3-cp311-none-win_amd64.whl (138 kB)\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/138.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/138.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/138.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/138.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/138.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/138.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/138.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/138.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/138.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/138.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/138.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/138.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/138.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/138.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/138.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/138.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/138.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/138.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/138.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/138.8 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 30.7/138.8 kB 25.7 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 30.7/138.8 kB 25.7 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 30.7/138.8 kB 25.7 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 30.7/138.8 kB 25.7 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 30.7/138.8 kB 25.7 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 30.7/138.8 kB 25.7 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 30.7/138.8 kB 25.7 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 30.7/138.8 kB 25.7 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 30.7/138.8 kB 25.7 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 30.7/138.8 kB 25.7 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 30.7/138.8 kB 25.7 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 30.7/138.8 kB 25.7 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 30.7/138.8 kB 25.7 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 41.0/138.8 kB 22.6 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 41.0/138.8 kB 22.6 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 41.0/138.8 kB 22.6 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 41.0/138.8 kB 22.6 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 41.0/138.8 kB 22.6 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 41.0/138.8 kB 22.6 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 41.0/138.8 kB 22.6 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 41.0/138.8 kB 22.6 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 41.0/138.8 kB 22.6 kB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 61.4/138.8 kB 28.7 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 61.4/138.8 kB 28.7 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 61.4/138.8 kB 28.7 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 61.4/138.8 kB 28.7 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 61.4/138.8 kB 28.7 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 61.4/138.8 kB 28.7 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 61.4/138.8 kB 28.7 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 61.4/138.8 kB 28.7 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 61.4/138.8 kB 28.7 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 61.4/138.8 kB 28.7 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 61.4/138.8 kB 28.7 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 61.4/138.8 kB 28.7 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 61.4/138.8 kB 28.7 kB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 81.9/138.8 kB 30.8 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 81.9/138.8 kB 30.8 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 81.9/138.8 kB 30.8 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 92.2/138.8 kB 33.4 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 92.2/138.8 kB 33.4 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 92.2/138.8 kB 33.4 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 92.2/138.8 kB 33.4 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 92.2/138.8 kB 33.4 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 92.2/138.8 kB 33.4 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 92.2/138.8 kB 33.4 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 92.2/138.8 kB 33.4 kB/s eta 0:00:02\n",
      "   ------------------------------- ------- 112.6/138.8 kB 36.8 kB/s eta 0:00:01\n",
      "   ------------------------------- ------- 112.6/138.8 kB 36.8 kB/s eta 0:00:01\n",
      "   ------------------------------- ------- 112.6/138.8 kB 36.8 kB/s eta 0:00:01\n",
      "   ------------------------------- ------- 112.6/138.8 kB 36.8 kB/s eta 0:00:01\n",
      "   ------------------------------- ------- 112.6/138.8 kB 36.8 kB/s eta 0:00:01\n",
      "   ------------------------------- ------- 112.6/138.8 kB 36.8 kB/s eta 0:00:01\n",
      "   ------------------------------- ------- 112.6/138.8 kB 36.8 kB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 122.9/138.8 kB 36.2 kB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 122.9/138.8 kB 36.2 kB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 122.9/138.8 kB 36.2 kB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 122.9/138.8 kB 36.2 kB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 122.9/138.8 kB 36.2 kB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 122.9/138.8 kB 36.2 kB/s eta 0:00:01\n",
      "   ------------------------------------- - 133.1/138.8 kB 36.7 kB/s eta 0:00:01\n",
      "   --------------------------------------- 138.8/138.8 kB 38.1 kB/s eta 0:00:00\n",
      "Using cached python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Using cached starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 0.0/62.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/62.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/62.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/62.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/62.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/62.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/62.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/62.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/62.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/62.4 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 10.2/62.4 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 10.2/62.4 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 10.2/62.4 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 10.2/62.4 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 10.2/62.4 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 10.2/62.4 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 10.2/62.4 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 10.2/62.4 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 10.2/62.4 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 30.7/62.4 kB 52.4 kB/s eta 0:00:01\n",
      "   ------------------- -------------------- 30.7/62.4 kB 52.4 kB/s eta 0:00:01\n",
      "   ------------------- -------------------- 30.7/62.4 kB 52.4 kB/s eta 0:00:01\n",
      "   ------------------- -------------------- 30.7/62.4 kB 52.4 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 41.0/62.4 kB 56.2 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 41.0/62.4 kB 56.2 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 41.0/62.4 kB 56.2 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 41.0/62.4 kB 56.2 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 41.0/62.4 kB 56.2 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 41.0/62.4 kB 56.2 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 41.0/62.4 kB 56.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  61.4/62.4 kB 59.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 62.4/62.4 kB 57.5 kB/s eta 0:00:00\n",
      "Using cached dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading httptools-0.6.1-cp311-cp311-win_amd64.whl (55 kB)\n",
      "   ---------------------------------------- 0.0/55.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/55.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/55.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/55.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/55.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/55.4 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 10.2/55.4 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 10.2/55.4 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 10.2/55.4 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 10.2/55.4 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 10.2/55.4 kB ? eta -:--:--\n",
      "   ---------------------- ----------------- 30.7/55.4 kB 100.4 kB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 30.7/55.4 kB 100.4 kB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 30.7/55.4 kB 100.4 kB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 30.7/55.4 kB 100.4 kB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 30.7/55.4 kB 100.4 kB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 30.7/55.4 kB 100.4 kB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 30.7/55.4 kB 100.4 kB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 30.7/55.4 kB 100.4 kB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 30.7/55.4 kB 100.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 51.2/55.4 kB 69.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 55.4/55.4 kB 72.3 kB/s eta 0:00:00\n",
      "Using cached typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "Downloading watchfiles-0.22.0-cp311-none-win_amd64.whl (281 kB)\n",
      "   ---------------------------------------- 0.0/282.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/282.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/282.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/282.0 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/282.0 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/282.0 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/282.0 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/282.0 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/282.0 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/282.0 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/282.0 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/282.0 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/282.0 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 30.7/282.0 kB 59.5 kB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 30.7/282.0 kB 59.5 kB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 30.7/282.0 kB 59.5 kB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 30.7/282.0 kB 59.5 kB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 30.7/282.0 kB 59.5 kB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 30.7/282.0 kB 59.5 kB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 41.0/282.0 kB 51.7 kB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 41.0/282.0 kB 51.7 kB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 41.0/282.0 kB 51.7 kB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 41.0/282.0 kB 51.7 kB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 41.0/282.0 kB 51.7 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 61.4/282.0 kB 61.8 kB/s eta 0:00:04\n",
      "   -------- ------------------------------- 61.4/282.0 kB 61.8 kB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 81.9/282.0 kB 77.7 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 81.9/282.0 kB 77.7 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 81.9/282.0 kB 77.7 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 81.9/282.0 kB 77.7 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 81.9/282.0 kB 77.7 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 81.9/282.0 kB 77.7 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 92.2/282.0 kB 69.0 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 92.2/282.0 kB 69.0 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 92.2/282.0 kB 69.0 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 92.2/282.0 kB 69.0 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 92.2/282.0 kB 69.0 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 92.2/282.0 kB 69.0 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 92.2/282.0 kB 69.0 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 92.2/282.0 kB 69.0 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 92.2/282.0 kB 69.0 kB/s eta 0:00:03\n",
      "   ---------------- ---------------------- 122.9/282.0 kB 73.5 kB/s eta 0:00:03\n",
      "   ---------------- ---------------------- 122.9/282.0 kB 73.5 kB/s eta 0:00:03\n",
      "   ---------------- ---------------------- 122.9/282.0 kB 73.5 kB/s eta 0:00:03\n",
      "   ---------------- ---------------------- 122.9/282.0 kB 73.5 kB/s eta 0:00:03\n",
      "   ---------------- ---------------------- 122.9/282.0 kB 73.5 kB/s eta 0:00:03\n",
      "   ---------------- ---------------------- 122.9/282.0 kB 73.5 kB/s eta 0:00:03\n",
      "   ---------------- ---------------------- 122.9/282.0 kB 73.5 kB/s eta 0:00:03\n",
      "   ---------------- ---------------------- 122.9/282.0 kB 73.5 kB/s eta 0:00:03\n",
      "   ---------------- ---------------------- 122.9/282.0 kB 73.5 kB/s eta 0:00:03\n",
      "   ---------------- ---------------------- 122.9/282.0 kB 73.5 kB/s eta 0:00:03\n",
      "   ---------------- ---------------------- 122.9/282.0 kB 73.5 kB/s eta 0:00:03\n",
      "   ---------------- ---------------------- 122.9/282.0 kB 73.5 kB/s eta 0:00:03\n",
      "   ---------------- ---------------------- 122.9/282.0 kB 73.5 kB/s eta 0:00:03\n",
      "   ---------------- ---------------------- 122.9/282.0 kB 73.5 kB/s eta 0:00:03\n",
      "   ---------------- ---------------------- 122.9/282.0 kB 73.5 kB/s eta 0:00:03\n",
      "   ------------------------ -------------- 174.1/282.0 kB 75.4 kB/s eta 0:00:02\n",
      "   ------------------------ -------------- 174.1/282.0 kB 75.4 kB/s eta 0:00:02\n",
      "   ------------------------ -------------- 174.1/282.0 kB 75.4 kB/s eta 0:00:02\n",
      "   ------------------------ -------------- 174.1/282.0 kB 75.4 kB/s eta 0:00:02\n",
      "   ------------------------ -------------- 174.1/282.0 kB 75.4 kB/s eta 0:00:02\n",
      "   ------------------------ -------------- 174.1/282.0 kB 75.4 kB/s eta 0:00:02\n",
      "   -------------------------- ------------ 194.6/282.0 kB 76.6 kB/s eta 0:00:02\n",
      "   -------------------------- ------------ 194.6/282.0 kB 76.6 kB/s eta 0:00:02\n",
      "   -------------------------- ------------ 194.6/282.0 kB 76.6 kB/s eta 0:00:02\n",
      "   -------------------------- ------------ 194.6/282.0 kB 76.6 kB/s eta 0:00:02\n",
      "   -------------------------- ------------ 194.6/282.0 kB 76.6 kB/s eta 0:00:02\n",
      "   -------------------------- ------------ 194.6/282.0 kB 76.6 kB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 204.8/282.0 kB 72.0 kB/s eta 0:00:02\n",
      "   ------------------------------- ------- 225.3/282.0 kB 78.6 kB/s eta 0:00:01\n",
      "   ------------------------------- ------- 225.3/282.0 kB 78.6 kB/s eta 0:00:01\n",
      "   ------------------------------- ------- 225.3/282.0 kB 78.6 kB/s eta 0:00:01\n",
      "   ------------------------------- ------- 225.3/282.0 kB 78.6 kB/s eta 0:00:01\n",
      "   --------------------------------- ----- 245.8/282.0 kB 81.0 kB/s eta 0:00:01\n",
      "   --------------------------------- ----- 245.8/282.0 kB 81.0 kB/s eta 0:00:01\n",
      "   --------------------------------- ----- 245.8/282.0 kB 81.0 kB/s eta 0:00:01\n",
      "   --------------------------------- ----- 245.8/282.0 kB 81.0 kB/s eta 0:00:01\n",
      "   --------------------------------- ----- 245.8/282.0 kB 81.0 kB/s eta 0:00:01\n",
      "   --------------------------------- ----- 245.8/282.0 kB 81.0 kB/s eta 0:00:01\n",
      "   --------------------------------- ----- 245.8/282.0 kB 81.0 kB/s eta 0:00:01\n",
      "   --------------------------------- ----- 245.8/282.0 kB 81.0 kB/s eta 0:00:01\n",
      "   --------------------------------- ----- 245.8/282.0 kB 81.0 kB/s eta 0:00:01\n",
      "   ----------------------------------- --- 256.0/282.0 kB 75.6 kB/s eta 0:00:01\n",
      "   --------------------------------------  276.5/282.0 kB 80.8 kB/s eta 0:00:01\n",
      "   --------------------------------------  276.5/282.0 kB 80.8 kB/s eta 0:00:01\n",
      "   --------------------------------------  276.5/282.0 kB 80.8 kB/s eta 0:00:01\n",
      "   --------------------------------------- 282.0/282.0 kB 78.7 kB/s eta 0:00:00\n",
      "Downloading websockets-12.0-cp311-cp311-win_amd64.whl (124 kB)\n",
      "   ---------------------------------------- 0.0/125.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/125.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/125.0 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 10.2/125.0 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 10.2/125.0 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 10.2/125.0 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 10.2/125.0 kB ? eta -:--:--\n",
      "   --------- ----------------------------- 30.7/125.0 kB 108.9 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 30.7/125.0 kB 108.9 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 30.7/125.0 kB 108.9 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 30.7/125.0 kB 108.9 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 30.7/125.0 kB 108.9 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 30.7/125.0 kB 108.9 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 30.7/125.0 kB 108.9 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 30.7/125.0 kB 108.9 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 30.7/125.0 kB 108.9 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 30.7/125.0 kB 108.9 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 30.7/125.0 kB 108.9 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 30.7/125.0 kB 108.9 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 30.7/125.0 kB 108.9 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 30.7/125.0 kB 108.9 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 30.7/125.0 kB 108.9 kB/s eta 0:00:01\n",
      "   ------------- -------------------------- 41.0/125.0 kB 38.5 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 41.0/125.0 kB 38.5 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 41.0/125.0 kB 38.5 kB/s eta 0:00:03\n",
      "   ------------------- -------------------- 61.4/125.0 kB 54.6 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 61.4/125.0 kB 54.6 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 61.4/125.0 kB 54.6 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 61.4/125.0 kB 54.6 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 61.4/125.0 kB 54.6 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 61.4/125.0 kB 54.6 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 61.4/125.0 kB 54.6 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 81.9/125.0 kB 57.3 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 81.9/125.0 kB 57.3 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 81.9/125.0 kB 57.3 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 81.9/125.0 kB 57.3 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 81.9/125.0 kB 57.3 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 81.9/125.0 kB 57.3 kB/s eta 0:00:01\n",
      "   ----------------------------------- --- 112.6/125.0 kB 68.3 kB/s eta 0:00:01\n",
      "   ----------------------------------- --- 112.6/125.0 kB 68.3 kB/s eta 0:00:01\n",
      "   ----------------------------------- --- 112.6/125.0 kB 68.3 kB/s eta 0:00:01\n",
      "   ----------------------------------- --- 112.6/125.0 kB 68.3 kB/s eta 0:00:01\n",
      "   ----------------------------------- --- 112.6/125.0 kB 68.3 kB/s eta 0:00:01\n",
      "   --------------------------------------  122.9/125.0 kB 65.5 kB/s eta 0:00:01\n",
      "   --------------------------------------- 125.0/125.0 kB 65.6 kB/s eta 0:00:00\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Installing collected packages: websockets, shellingham, python-multipart, orjson, httptools, h11, dnspython, watchfiles, uvicorn, starlette, httpcore, email_validator, typer, httpx, fastapi-cli, fastapi\n",
      "Successfully installed dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 orjson-3.10.3 python-multipart-0.0.9 shellingham-1.5.4 starlette-0.37.2 typer-0.12.3 uvicorn-0.30.1 watchfiles-0.22.0 websockets-12.0\n"
     ]
    }
   ],
   "source": [
    "! pip install fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce2123d2-d3bf-410a-9fce-24b2f8421c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 124\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(e)}\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 124\u001b[0m     uvicorn\u001b[38;5;241m.\u001b[39mrun(app, host\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m127.0.0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, port\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8001\u001b[39m)\n",
      "File \u001b[1;32mD:\\annkonda2\\Lib\\site-packages\\uvicorn\\main.py:577\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(app, host, port, uds, fd, loop, http, ws, ws_max_size, ws_max_queue, ws_ping_interval, ws_ping_timeout, ws_per_message_deflate, lifespan, interface, reload, reload_dirs, reload_includes, reload_excludes, reload_delay, workers, env_file, log_config, log_level, access_log, proxy_headers, server_header, date_header, forwarded_allow_ips, root_path, limit_concurrency, backlog, limit_max_requests, timeout_keep_alive, timeout_graceful_shutdown, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_version, ssl_cert_reqs, ssl_ca_certs, ssl_ciphers, headers, use_colors, app_dir, factory, h11_max_incomplete_event_size)\u001b[0m\n\u001b[0;32m    575\u001b[0m         Multiprocess(config, target\u001b[38;5;241m=\u001b[39mserver\u001b[38;5;241m.\u001b[39mrun, sockets\u001b[38;5;241m=\u001b[39m[sock])\u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 577\u001b[0m         server\u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39muds \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(config\u001b[38;5;241m.\u001b[39muds):\n",
      "File \u001b[1;32mD:\\annkonda2\\Lib\\site-packages\\uvicorn\\server.py:65\u001b[0m, in \u001b[0;36mServer.run\u001b[1;34m(self, sockets)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, sockets: \u001b[38;5;28mlist\u001b[39m[socket\u001b[38;5;241m.\u001b[39msocket] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msetup_event_loop()\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserve(sockets\u001b[38;5;241m=\u001b[39msockets))\n",
      "File \u001b[1;32mD:\\annkonda2\\Lib\\asyncio\\runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, Request\n",
    "import uvicorn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from fastapi import FastAPI, Request\n",
    "import uvicorn\n",
    "print(\"2\")\n",
    "# كلاس لتحويل النصوص إلى متجهات\n",
    "class TextIndexer:\n",
    " \n",
    "\n",
    " def clean_space(column1 , column2,column3,column4):\n",
    "        column1 = column1.fillna('')\n",
    "        column2 = column2.fillna('')\n",
    "        column3= column3.fillna('')\n",
    "        column4 = column4.fillna('')\n",
    "        return column1,column2 ,column3 ,column4\n",
    "\n",
    "\n",
    "# إعداد FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "# إنشاء كائن من TextIndexer\n",
    "indexer = TextIndexer()\n",
    "###################    The first dataset ############################33333333\n",
    "\n",
    "@app.post(\"/index_texts\")\n",
    "async def index_texts(request: Request):\n",
    "    try:\n",
    "        print(\"start\")\n",
    "        # استخراج البيانات من الطلب\n",
    "        data = await request.json()\n",
    "        \n",
    "        data_file = data.get(\"cleanned_data_file\")\n",
    "\n",
    "        data_texts = read_cleaned_texts(data_file)\n",
    "         \n",
    "        if  data_texts is None :\n",
    "            return {\"error\": \" 'data' is fields are required.\"}\n",
    "       \n",
    "        vectorizer = TfidfVectorizer()\n",
    "        # تحويل النصوص إلى متجهات\n",
    "        data_vectors = vectorizer.fit_transform( data_texts)\n",
    "       \n",
    "        data_vector_file=r\"E:\\DocumentVector.pkl\"\n",
    "        vectorizer_file= r\"E:\\TfidfVector.pkl\"\n",
    "        save_vectors_to_binary_file(data_vectors ,data_vector_file)\n",
    "        \n",
    "        save_vectors_to_binary_file(vectorizer ,vectorizer_file)\n",
    "        \n",
    "        return {\n",
    "            \"data_vectors_file\": data_vector_file,\n",
    "            \"vectorizer_file\": vectorizer_file,\n",
    "            \n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "################# The secound dataset ########################\n",
    "\n",
    "@app.post(\"/index_texts_cli\")\n",
    "async def index_texts(request: Request):\n",
    "    try:\n",
    "       \n",
    "        print(\"start\")\n",
    "        # استخراج البيانات من الطلب\n",
    "        data = await request.json()\n",
    "    \n",
    "        # print(data)\n",
    "        column1 = data.get(\"column1\")\n",
    "        column2 = data.get(\"column2\")\n",
    "        column3 = data.get(\"column3\")\n",
    "        column4 = data.get(\"column4\")\n",
    "        \n",
    "        vectorizer = TfidfVectorizer()\n",
    "       \n",
    "        column1_vec = vectorizer.fit_transform(column1)\n",
    "        column2_vec = vectorizer.fit_transform(column2)\n",
    "        column3_vec = vectorizer.fit_transform(column3)\n",
    "        column4_vec = vectorizer.fit_transform(column4)\n",
    "\n",
    "        combined_vocabulary = set(vectorizer.vocabulary_.keys())\n",
    "      \n",
    "        # إعادة بناء vectorizer بناءً على المفردات المدمجة\n",
    "        combined_vocabulary = list(combined_vocabulary)\n",
    "        combined_vectorizer = TfidfVectorizer(vocabulary=combined_vocabulary)\n",
    "    \n",
    "        # إعادة تحويل النصوص باستخدام الـ vectorizer المدمج\n",
    "        column1_vec = combined_vectorizer.fit_transform(column1)\n",
    "        column2_vec = combined_vectorizer.fit_transform(column2)\n",
    "        column3_vec = combined_vectorizer.fit_transform(column3)\n",
    "        column4_vec = combined_vectorizer.fit_transform(column4)\n",
    "        \n",
    "        col1_weight = 4.0  \n",
    "        col4_weight = 1.5\n",
    "        # حساب المتوسط المرجح\n",
    "        weighted_col1_tfidf = col1_weight * column1_vec\n",
    "       \n",
    "        weighted_col4_tfidf = col4_weight * column4_vec\n",
    "        # جمع المصفوفات\n",
    "        sum_tfidf = weighted_col1_tfidf + column2_vec + weighted_col4_tfidf + column4_vec\n",
    "\n",
    "        # حساب المتوسط المرجح\n",
    "        average_tfidf = sum_tfidf / (col1_weight + 1 + col4_weight + 1)\n",
    "       \n",
    "              \n",
    "        data_vector_file=r\"E:\\DocumentVector_clinical.pkl\"\n",
    "        vectorizer_file= r\"E:\\TfidfVector_clinical.pkl\"\n",
    "\n",
    "        save_vectors_to_binary_file(average_tfidf ,data_vector_file)\n",
    "       \n",
    "        save_vectors_to_binary_file(combined_vectorizer ,vectorizer_file)\n",
    "        \n",
    "        return {\n",
    "            \"data_vectors_file\": data_vector_file,\n",
    "            \"vectorizer_file\": vectorizer_file,\n",
    "            \n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a502ff2-790d-4752-a185-e100bb434c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import lil_matrix\n",
    "print(\"3\")\n",
    "\n",
    "def get_documents_for_query(query, data_vector_file,  vectorizer_file, data_file):\n",
    "    with open(data_vector_file, \"rb\") as file:\n",
    "        data_vector = pickle.load(file)\n",
    "    with open(vectorizer_file, \"rb\") as file:\n",
    "        vectorizer = pickle.load(file)\n",
    "    data = load_dataset(data_file)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    cosine_similarities = cosine_similarity(data_vector, query_vector).flatten()\n",
    "   \n",
    "    n = 10\n",
    "    top_documents_indices = np.argsort(cosine_similarities)[-n:][::-1]\n",
    "    top_documents = data.iloc[top_documents_indices]\n",
    "    return top_documents , cosine_similarities[top_documents_indices]\n",
    "\n",
    "def read_cleaned_texts(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r' , encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            return [line.strip() for line in lines]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {file_path} does not exist.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def load_and_combine_vectors(file_paths):\n",
    "    all_vectors = []\n",
    "    for file_path in file_paths:\n",
    "        part_vectors = np.load(file_path)\n",
    "        all_vectors.append(part_vectors)\n",
    "    \n",
    "    # التأكد من تطابق الأبعاد\n",
    "    max_length = max(len(vec[0]) for vec in all_vectors)\n",
    "    all_vectors_padded = []\n",
    "    \n",
    "    for vec in all_vectors:\n",
    "        padded_vec = np.pad(vec, ((0, 0), (0, max_length - vec.shape[1])), 'constant')\n",
    "        all_vectors_padded.append(padded_vec)\n",
    "    \n",
    "    combined_vectors = np.vstack(all_vectors_padded)\n",
    "    return combined_vectors\n",
    "\n",
    "def process_text_via_service(text):\n",
    "    # print(\"Sending text to service:\", text)\n",
    "    url = \"http://127.0.0.1:8000/process_text\"\n",
    "    payload = {\"text\": text}\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    print(\"66666\")\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    print(\"77777\")\n",
    "    if response.status_code == 200:\n",
    "        print(\"Request successful!\")\n",
    "        return response.json().get(\"processed_text\")\n",
    "    else:\n",
    "        print(\"Request failed!\")\n",
    "        raise Exception(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
    "\n",
    "\n",
    "  \n",
    "def index_texts_via_service(cleanned_data_file):\n",
    "    print(\"Sending data and query to indexing service\")\n",
    "    url = \"http://127.0.0.1:8001/index_texts\"\n",
    "    payload = {\"cleanned_data_file\": cleanned_data_file, }\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(\"Indexing request successful!\")\n",
    "        response_json = response.json()\n",
    "        return response_json.get(\"data_vectors_file\") , response_json.get(\"vectorizer_file\")\n",
    "    else:\n",
    "        print(\"Indexing request failed!\")\n",
    "        raise Exception(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
    "\n",
    "\n",
    "def index_texts_via_service_cli(column1 , column2 , column3 ,column4):\n",
    "    print(\"Sending data and query to indexing service\")\n",
    "    url = \"http://127.0.0.1:8001/index_texts_cli\"\n",
    "    column1 = column1.fillna('')\n",
    "    column2 = column2.fillna('')\n",
    "    column3= column3.fillna('')\n",
    "    column4 = column4.fillna('')\n",
    "    \n",
    "    payload = {\"column1\": column1.tolist(), \n",
    "                \"column2\": column2.tolist(), \n",
    "                \"column3\": column3.tolist(), \n",
    "                \"column4\": column4.tolist()\n",
    "               }\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    print(\"111111\")\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    print(\"222222\")\n",
    "    if response.status_code == 200:\n",
    "        print(\"Indexing request successful!\")\n",
    "        response_json = response.json()\n",
    "        return response_json.get(\"data_vectors_file\") , response_json.get(\"vectorizer_file\")\n",
    "    else:\n",
    "        print(\"Indexing request failed!\")\n",
    "        raise Exception(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
    "\n",
    "\n",
    "    \n",
    "def calculate_similarity_via_service(query, tfidf_matrix_file, vectorizer_file, data_file):\n",
    "    print(\"Sending vectors to similarity service\")\n",
    "    url = \"http://127.0.0.1:8002/calculate_similarity\"\n",
    "    payload = {\n",
    "        \"data\": data_file ,\n",
    "         \"query\": query ,\n",
    "         \"data_vector\" :tfidf_matrix_file ,\n",
    "         \"vectorizer\" :vectorizer_file\n",
    "        }\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    print(\"iiiiiii\")\n",
    "    if response.status_code == 200:\n",
    "        print(\"Similarity calculation request successful!\")\n",
    "        return response.json().get(\"top_documents\")\n",
    "    else:\n",
    "        print(\"Similarity calculation request failed!\")\n",
    "        raise Exception(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
    "\n",
    "def calculate_similarity_via_service_cli(query, tfidf_matrix_file, vectorizer_file, data_file):\n",
    "    print(\"Sending vectors to similarity service\")\n",
    "    url = \"http://127.0.0.1:8002/calculate_similarity_cli\"\n",
    "    payload = {\n",
    "        \"data\": data_file ,\n",
    "         \"query\": query ,\n",
    "         \"data_vector\" :tfidf_matrix_file ,\n",
    "         \"vectorizer\" :vectorizer_file\n",
    "        }\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    print(\"iiiiiii\")\n",
    "    if response.status_code == 200:\n",
    "        print(\"Similarity calculation request successful!\")\n",
    "        return response.json().get(\"top_documents\")\n",
    "    else:\n",
    "        print(\"Similarity calculation request failed!\")\n",
    "        raise Exception(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "498301aa-c8c1-42a3-86d7-09801a881c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def sanitize_data(data):\n",
    "    if isinstance(data, float) and (math.isnan(data) or math.isinf(data)):\n",
    "        return None  # يمكنك استبدالها بقيمة افتراضية إذا لزم الأمر\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def extract_doc_ids(query_id, df):\n",
    "    query_id = int(query_id)\n",
    "    # قم بتصفية DataFrame باستخدام رقم الاستعلام وشرط relevance\n",
    "    filtered_df = df[(df['query_id'] == query_id) & ((df['relevance'] == 1) | (df['relevance'] == 2))]\n",
    "    \n",
    "    # استخراج قيم doc_id وتحويلها إلى قائمة\n",
    "    doc_ids = filtered_df['doc_id'].tolist()\n",
    "    relevent =filtered_df['relevance'].tolist()\n",
    "    return doc_ids , relevent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_documents_for_query2(query: str, data_vector_file: str, vectorizer_file: str, data_file: str):\n",
    "        try:\n",
    "            print(\"dddddddddddddddd\")\n",
    "            with open(data_vector_file, \"rb\") as file:\n",
    "                data_vector = pickle.load(file)\n",
    "            print(\"ffffffffffffffffffffff\")\n",
    "            with open(vectorizer_file, \"rb\") as file:\n",
    "                vectorizer = pickle.load(file)\n",
    "            print(\"pppppppppppppppp\")\n",
    "            data = load_dataset2(data_file)\n",
    "            print(\"eeeeeeeeeeeeeeeeeeeeeeeeee\")\n",
    "            query_vector = vectorizer.transform([query])\n",
    "            print(\"zzzzzzzzzzzzzzzzzzzzzzz\")\n",
    "            cosine_similarities = cosine_similarity(data_vector, query_vector).flatten().tolist()\n",
    "            print(\"hhhhhhhhhhhhhhhhhhhhhhh\")\n",
    "            cosine_similarities = [sanitize_data(x) for x in cosine_similarities]\n",
    "            print(\"llllllllllllllllllllllllllllllll\")\n",
    "            cosine_similarities = [int(x) for x in cosine_similarities]\n",
    "            n = 10\n",
    "            top_documents_indices = np.argsort(cosine_similarities)[-n:][::-1]\n",
    "            print(\"ttttttttttttttttttttttttttttttttttttt\")\n",
    "            top_documents = data.iloc[top_documents_indices]\n",
    "            print(\"sssssssssssssssssssssssssssssssssssssssssssssssss\")\n",
    "\n",
    "            return top_documents , cosine_similarities[top_documents_indices]\n",
    "         \n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4619da1-ecfa-440e-8c50-ae10484ade95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen genericpath>:77: RuntimeWarning: coroutine 'Server.serve' was never awaited\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'save_read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msave_read\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m precision_score, recall_score, average_precision_score\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTextProcessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextProcessor, processtext\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'save_read'"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, Request\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "from save_read import load_dataset\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
    "from TextProcessing import TextProcessor, processtext\n",
    "import uvicorn\n",
    "\n",
    "class Evaluation:\n",
    " \n",
    "    @staticmethod\n",
    "    def calculate_precision_recall(y_true, y_pred, threshold=0.5):\n",
    "        y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "        precision = precision_score(y_true, y_pred_binary, average='micro')\n",
    "        recall = recall_score(y_true, y_pred_binary, average='micro')\n",
    "        return precision, recall\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_map_score(y_true, y_pred):\n",
    "        return average_precision_score(y_true, y_pred, average='micro')\n",
    "    \n",
    "    # def calculate_mrr(y_true):\n",
    "    #      # Calculate reciprocal rank\n",
    "           \n",
    "    @staticmethod\n",
    "    def load_queries(queries_paths):\n",
    "        queries = []\n",
    "        for file_path in queries_paths:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                for line in file:\n",
    "                    try:\n",
    "                        query = json.loads(line.strip())\n",
    "                        if 'query' in query:\n",
    "                            queries.append(query)\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Skipping invalid line in {file_path}: {line}\")\n",
    "        return queries\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_texts(texts):\n",
    "        processed_texts = []\n",
    "        for text in texts:\n",
    "            #print(\"text: \" + text)\n",
    "            processed_text = processtext(text)\n",
    "            processed_texts.append(processed_text)\n",
    "        return processed_texts\n",
    "    \n",
    "    def read_csv_to_array(file_path):\n",
    "        # قراءة ملف CSV باستخدام pandas\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # تحويل إطار البيانات إلى مصفوفة من القوائم\n",
    "        data_array = df.values.tolist()\n",
    "        \n",
    "        return data_array\n",
    "\n",
    "    @staticmethod\n",
    "    def get_documents_for_query_cli(query, tfidf_matrix, vectorizer, data):\n",
    "        with open(tfidf_matrix, \"rb\") as file:\n",
    "            tfidf_matrix = pickle.load(file)\n",
    "            \n",
    "        with open(vectorizer, \"rb\") as file:\n",
    "            vectorizer = pickle.load(file)\n",
    "            \n",
    "        data = load_dataset2(data)\n",
    "            \n",
    "        query_vector = vectorizer.transform([query])\n",
    "        cosine_similarities = cosine_similarity(tfidf_matrix, query_vector).flatten()\n",
    "        n = 10\n",
    "        top_documents_indices = cosine_similarities.argsort()[-n:][::-1]\n",
    "        top_documents = data.iloc[top_documents_indices]\n",
    "    \n",
    "        return top_documents, cosine_similarities[top_documents_indices]\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluation(queries_file, tfidf_matrix_file, vectorizer_file, data_file):\n",
    "            queries = Evaluation.load_queries([queries_file])\n",
    "            all_precisions = []\n",
    "            all_recalls = []\n",
    "            all_map_scores = []\n",
    "            all_rr = []\n",
    "            \n",
    "            for query in queries:\n",
    "                if 'query' in query:\n",
    "                    processed_query = Evaluation.process_texts([query['query']])[0]\n",
    "                    print(processed_query)\n",
    "                    processed_query = processed_query['processed_text']\n",
    "                    top_documents, cosine_similarities = get_documents_for_query(processed_query, tfidf_matrix_file, vectorizer_file, data_file)\n",
    "                    \n",
    "                    data = load_dataset(data_file)\n",
    "                    relevance = np.zeros(len(data))\n",
    "                    for pid in query.get('answer_pids', []):\n",
    "                        relevance[np.where(data['pid'] == pid)[0]] = 1\n",
    "\n",
    "                    y_true = relevance[top_documents.index]\n",
    "                    y_pred = cosine_similarities\n",
    "\n",
    "                    if y_true.sum() == 0:\n",
    "                        print(f\"No relevant documents for query ID: {query.get('qid', 'N/A')}\")\n",
    "                        # print(\"Documents found:\", top_documents)\n",
    "                        continue\n",
    "\n",
    "                    precision, recall = Evaluation.calculate_precision_recall(y_true, y_pred)\n",
    "                    all_precisions.append(precision)\n",
    "                    all_recalls.append(recall)\n",
    "\n",
    "                    map_score = Evaluation.calculate_map_score(y_true, y_pred)\n",
    "                    all_map_scores.append(map_score)\n",
    "                    # Calculate reciprocal rank\n",
    "                    rr = 0\n",
    "                    for i, rel in enumerate(y_true):\n",
    "                        if rel == 1:\n",
    "                            rr = 1 / (i + 1)  # Reciprocal rank\n",
    "                            break\n",
    "                    all_rr.append(rr)\n",
    "                    print(f\"Query ID: {query.get('qid', 'N/A')}, Precision: {precision}, Recall: {recall}, MAP Score: {map_score}, RR: {rr}\")\n",
    "                    # print(\n",
    "                    #     f\"Query ID: {query.get('qid', 'N/A')}, Precision: {precision}, Recall: {recall}, MAP Score: {map_score}, RR: {rr}\")\n",
    "\n",
    "        \n",
    "            avg_precision = np.mean(all_precisions)\n",
    "            avg_recall = np.mean(all_recalls)\n",
    "            avg_map_score = np.mean(all_map_scores)\n",
    "            mrr = np.mean(all_rr)\n",
    "            # print(f\"Average Precision: {avg_precision}, Average Recall: {avg_recall}, Average MAP Score: {avg_map_score}, MRR: {mrr}\")\n",
    "            # return avg_precision ,avg_recall ,avg_map_score\n",
    "            print(f\"Average Precision: {avg_precision}, Average Recall: {avg_recall}, Average MAP Score: {avg_map_score}, MRR: {mrr}\")       \n",
    "\n",
    "    @staticmethod\n",
    "    def evaluation_clic(queries_file, tfidf_matrix_file, vectorizer_file, data_file):\n",
    "            print(\"111111111111111\")\n",
    "            queries = read_csv_to_array(queries_file)\n",
    "            all_precisions = []\n",
    "            all_recalls = []\n",
    "            all_map_scores = []\n",
    "            all_rr = []\n",
    "            print(\"22222222222\")\n",
    "            for query in queries:\n",
    "                query = [str(item) for item in query]\n",
    "                combined_query = ' '.join(query)\n",
    "                id=query[0]  \n",
    "                print(\"333333\")\n",
    "                top_documents, cosine_similarities = Evaluation.get_documents_for_query_cli(combined_query, tfidf_matrix_file,vectorizer_file, data_file)\n",
    "                qrles=r\"D:\\qrels.csv\"\n",
    "                df = pd.read_csv(qrles)\n",
    "                doc_ids , relevence = extract_doc_ids(id ,df)\n",
    "                modfied_data=r\"D:\\modified_dataset.csv\"\n",
    "                data = pd.read_csv(modfied_data)\n",
    "                relevance = np.zeros(len(data))\n",
    "                for doc_id in doc_ids:\n",
    "                        relevance[np.where(data['doc_id'] == doc_id)[0]] = 1\n",
    "\n",
    "                y_true = relevance[top_documents.index]\n",
    "                y_pred = cosine_similarities\n",
    "\n",
    "                if y_true.sum() == 0:\n",
    "                        print(f\"No relevant documents for query ID: {query[0] }\")\n",
    "                        continue\n",
    "\n",
    "                precision, recall =Evaluation.calculate_precision_recall(y_true, y_pred)\n",
    "                all_precisions.append(precision)\n",
    "                all_recalls.append(recall)\n",
    "\n",
    "                map_score = Evaluation.calculate_map_score(y_true, y_pred)\n",
    "                all_map_scores.append(map_score)\n",
    "                rr = 0\n",
    "                for i, rel in enumerate(y_true):\n",
    "                    if rel == 1:\n",
    "                        rr = 1 / (i + 1)  # Reciprocal rank\n",
    "                        break\n",
    "                all_rr.append(rr)\n",
    "\n",
    "                print(f\"Query ID: {query[0] }, Precision: {precision}, Recall: {recall}, MAP Score: {map_score}, RR: {rr}\")\n",
    "\n",
    "            avg_precision = np.mean(all_precisions)\n",
    "            avg_recall = np.mean(all_recalls)\n",
    "            avg_map_score = np.mean(all_map_scores)\n",
    "            mrr = np.mean(all_rr)\n",
    "\n",
    "            print(f\"Average Precision: {avg_precision}, Average Recall: {avg_recall}, Average MAP Score: {avg_map_score}, MRR: {mrr}\")\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "evaluation =Evaluation()\n",
    "\n",
    "yourdataset=r\"C:\\Users\\sayas\\.ir_datasets\\lotte\\lotte_extracted\\lotte\\lifestyle\\dev\\try.tsv\"\n",
    "\n",
    "\n",
    "process_queries=r\"D:\\processed_queries.csv\"\n",
    "\n",
    "\n",
    "data_vector_file_cli= r\"D:\\DocumentVector_clinical.pkl\"\n",
    "\n",
    "vectorizer_file_cli= r\"D:\\TfidfVector_clinical.pkl\"  \n",
    "\n",
    "Evaluation.evaluation_clic(process_queries,data_vector_file_cli,vectorizer_file_cli,yourdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c6e22f6-f9dc-421a-a461-33013b726171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "111111111111111\n",
      "22222222222\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 1, Precision: 0.8, Recall: 0.8, MAP Score: 0.45, RR: 0.5\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 2, Precision: 0.5, Recall: 0.5, MAP Score: 0.6222222222222222, RR: 1.0\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 3, Precision: 0.7, Recall: 0.7, MAP Score: 0.7916666666666666, RR: 1.0\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 4, Precision: 0.9, Recall: 0.9, MAP Score: 0.14285714285714285, RR: 0.14285714285714285\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 5, Precision: 0.6, Recall: 0.6, MAP Score: 0.5416666666666666, RR: 0.5\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 6, Precision: 0.6, Recall: 0.6, MAP Score: 0.6791666666666667, RR: 0.5\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 7, Precision: 0.3, Recall: 0.3, MAP Score: 0.9087301587301586, RR: 1.0\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 8, Precision: 0.6, Recall: 0.6, MAP Score: 0.41944444444444445, RR: 0.3333333333333333\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 9, Precision: 0.6, Recall: 0.6, MAP Score: 1.0, RR: 1.0\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No relevant documents for query ID: 10\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 11, Precision: 0.6, Recall: 0.6, MAP Score: 0.875, RR: 1.0\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 12, Precision: 0.7, Recall: 0.7, MAP Score: 0.6555555555555554, RR: 1.0\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No relevant documents for query ID: 13\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 14, Precision: 0.9, Recall: 0.9, MAP Score: 1.0, RR: 1.0\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 15, Precision: 0.9, Recall: 0.9, MAP Score: 1.0, RR: 1.0\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 16, Precision: 0.9, Recall: 0.9, MAP Score: 1.0, RR: 1.0\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No relevant documents for query ID: 17\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 18, Precision: 0.9, Recall: 0.9, MAP Score: 0.1111111111111111, RR: 0.1111111111111111\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 19, Precision: 0.9, Recall: 0.9, MAP Score: 0.125, RR: 0.125\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No relevant documents for query ID: 20\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 21, Precision: 0.7, Recall: 0.7, MAP Score: 0.35555555555555557, RR: 0.3333333333333333\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No relevant documents for query ID: 22\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No relevant documents for query ID: 23\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 24, Precision: 0.7, Recall: 0.7, MAP Score: 0.8055555555555556, RR: 1.0\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 25, Precision: 0.7, Recall: 0.7, MAP Score: 0.8666666666666667, RR: 1.0\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No relevant documents for query ID: 26\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No relevant documents for query ID: 27\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No relevant documents for query ID: 28\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No relevant documents for query ID: 29\n",
      "333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\annkonda2\\Lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 30, Precision: 0.9, Recall: 0.9, MAP Score: 1.0, RR: 1.0\n",
      "Average Precision: 0.72, Average Recall: 0.72, Average MAP Score: 0.6675099206349205, MRR: 0.727281746031746\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, delimiter='\\t', header=None, names=['pid', 'text'])\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"Error reading the dataset file: {e}\")\n",
    "        sys.exit(1)\n",
    "    return data\n",
    "\n",
    "def read_cleaned_texts(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r' , encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            return [line.strip() for line in lines]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {file_path} does not exist.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "def save_vectors_to_binary_file(vector, filename):\n",
    "        \n",
    "        with open(filename, \"wb\") as file:\n",
    "          pickle.dump(vector, file)\n",
    "\n",
    "def convert_csv_to_tsv(input_csv_path, output_tsv_path):\n",
    "    try:\n",
    "        # قراءة ملف CSV\n",
    "        df = pd.read_csv(input_csv_path)\n",
    "        \n",
    "        # حفظ البيانات في ملف TSV\n",
    "        df.to_csv(output_tsv_path, sep='\\t', index=False)\n",
    "        print(f\"تم تحويل الملف بنجاح وحفظه في {output_tsv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"حدث خطأ أثناء تحويل الملف: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset2(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, delimiter='\\t')\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"Error reading the dataset file: {e}\")\n",
    "        sys.exit(1)\n",
    "    return data\n",
    "\n",
    "def read_csv_to_array(file_path):\n",
    "    # قراءة ملف CSV باستخدام pandas\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # تحويل إطار البيانات إلى مصفوفة من القوائم\n",
    "    data_array = df.values.tolist()\n",
    "    \n",
    "    return data_array\n",
    "\n",
    "\n",
    "from fastapi import FastAPI, Request\n",
    "import uvicorn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from fastapi import FastAPI, Request\n",
    "import uvicorn\n",
    "print(\"2\")\n",
    "# كلاس لتحويل النصوص إلى متجهات\n",
    "class TextIndexer:\n",
    " \n",
    "\n",
    " def clean_space(column1 , column2,column3,column4):\n",
    "        column1 = column1.fillna('')\n",
    "        column2 = column2.fillna('')\n",
    "        column3= column3.fillna('')\n",
    "        column4 = column4.fillna('')\n",
    "        return column1,column2 ,column3 ,column4\n",
    "\n",
    "\n",
    "# إعداد FastAPI\n",
    "\n",
    "\n",
    "# إنشاء كائن من TextIndexer\n",
    "indexer = TextIndexer()\n",
    "###################    The first dataset ############################33333333\n",
    "\n",
    "\n",
    "\n",
    "################# The secound dataset ########################\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import lil_matrix\n",
    "print(\"3\")\n",
    "\n",
    "def get_documents_for_query(query, data_vector_file,  vectorizer_file, data_file):\n",
    "    with open(data_vector_file, \"rb\") as file:\n",
    "        data_vector = pickle.load(file)\n",
    "    with open(vectorizer_file, \"rb\") as file:\n",
    "        vectorizer = pickle.load(file)\n",
    "    data = load_dataset(data_file)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    cosine_similarities = cosine_similarity(data_vector, query_vector).flatten()\n",
    "   \n",
    "    n = 10\n",
    "    top_documents_indices = np.argsort(cosine_similarities)[-n:][::-1]\n",
    "    top_documents = data.iloc[top_documents_indices]\n",
    "    return top_documents , cosine_similarities[top_documents_indices]\n",
    "\n",
    "def read_cleaned_texts(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r' , encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            return [line.strip() for line in lines]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {file_path} does not exist.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def load_and_combine_vectors(file_paths):\n",
    "    all_vectors = []\n",
    "    for file_path in file_paths:\n",
    "        part_vectors = np.load(file_path)\n",
    "        all_vectors.append(part_vectors)\n",
    "    \n",
    "    # التأكد من تطابق الأبعاد\n",
    "    max_length = max(len(vec[0]) for vec in all_vectors)\n",
    "    all_vectors_padded = []\n",
    "    \n",
    "    for vec in all_vectors:\n",
    "        padded_vec = np.pad(vec, ((0, 0), (0, max_length - vec.shape[1])), 'constant')\n",
    "        all_vectors_padded.append(padded_vec)\n",
    "    \n",
    "    combined_vectors = np.vstack(all_vectors_padded)\n",
    "    return combined_vectors\n",
    "    \n",
    "\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def sanitize_data(data):\n",
    "    if isinstance(data, float) and (math.isnan(data) or math.isinf(data)):\n",
    "        return None  # يمكنك استبدالها بقيمة افتراضية إذا لزم الأمر\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def extract_doc_ids(query_id, df):\n",
    "    query_id = int(query_id)\n",
    "    # قم بتصفية DataFrame باستخدام رقم الاستعلام وشرط relevance\n",
    "    filtered_df = df[(df['query_id'] == query_id) & ((df['relevance'] == 1) | (df['relevance'] == 2))]\n",
    "    \n",
    "    # استخراج قيم doc_id وتحويلها إلى قائمة\n",
    "    doc_ids = filtered_df['doc_id'].tolist()\n",
    "    relevent =filtered_df['relevance'].tolist()\n",
    "    return doc_ids , relevent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_documents_for_query2(query: str, data_vector_file: str, vectorizer_file: str, data_file: str):\n",
    "        try:\n",
    "            print(\"dddddddddddddddd\")\n",
    "            with open(data_vector_file, \"rb\") as file:\n",
    "                data_vector = pickle.load(file)\n",
    "            print(\"ffffffffffffffffffffff\")\n",
    "            with open(vectorizer_file, \"rb\") as file:\n",
    "                vectorizer = pickle.load(file)\n",
    "            print(\"pppppppppppppppp\")\n",
    "            data = load_dataset2(data_file)\n",
    "            print(\"eeeeeeeeeeeeeeeeeeeeeeeeee\")\n",
    "            query_vector = vectorizer.transform([query])\n",
    "            print(\"zzzzzzzzzzzzzzzzzzzzzzz\")\n",
    "            cosine_similarities = cosine_similarity(data_vector, query_vector).flatten().tolist()\n",
    "            print(\"hhhhhhhhhhhhhhhhhhhhhhh\")\n",
    "            cosine_similarities = [sanitize_data(x) for x in cosine_similarities]\n",
    "            print(\"llllllllllllllllllllllllllllllll\")\n",
    "            cosine_similarities = [int(x) for x in cosine_similarities]\n",
    "            n = 10\n",
    "            top_documents_indices = np.argsort(cosine_similarities)[-n:][::-1]\n",
    "            print(\"ttttttttttttttttttttttttttttttttttttt\")\n",
    "            top_documents = data.iloc[top_documents_indices]\n",
    "            print(\"sssssssssssssssssssssssssssssssssssssssssssssssss\")\n",
    "\n",
    "            return top_documents , cosine_similarities[top_documents_indices]\n",
    "         \n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "            \n",
    "from fastapi import FastAPI, Request\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "# from save_read import load_dataset\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
    "# from TextProcessing import TextProcessor, processtext\n",
    "import uvicorn\n",
    "\n",
    "class Evaluation:\n",
    " \n",
    "    @staticmethod\n",
    "    def calculate_precision_recall(y_true, y_pred, threshold=0.5):\n",
    "        y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "        precision = precision_score(y_true, y_pred_binary, average='micro')\n",
    "        recall = recall_score(y_true, y_pred_binary, average='micro')\n",
    "        return precision, recall\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_map_score(y_true, y_pred):\n",
    "        return average_precision_score(y_true, y_pred, average='micro')\n",
    "    \n",
    "    # def calculate_mrr(y_true):\n",
    "    #      # Calculate reciprocal rank\n",
    "           \n",
    "    @staticmethod\n",
    "    def load_queries(queries_paths):\n",
    "        queries = []\n",
    "        for file_path in queries_paths:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                for line in file:\n",
    "                    try:\n",
    "                        query = json.loads(line.strip())\n",
    "                        if 'query' in query:\n",
    "                            queries.append(query)\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Skipping invalid line in {file_path}: {line}\")\n",
    "        return queries\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_texts(texts):\n",
    "        processed_texts = []\n",
    "        for text in texts:\n",
    "            #print(\"text: \" + text)\n",
    "            processed_text = processtext(text)\n",
    "            processed_texts.append(processed_text)\n",
    "        return processed_texts\n",
    "    \n",
    "    def read_csv_to_array(file_path):\n",
    "        # قراءة ملف CSV باستخدام pandas\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # تحويل إطار البيانات إلى مصفوفة من القوائم\n",
    "        data_array = df.values.tolist()\n",
    "        \n",
    "        return data_array\n",
    "\n",
    "    @staticmethod\n",
    "    def get_documents_for_query_cli(query, tfidf_matrix, vectorizer, data):\n",
    "        with open(tfidf_matrix, \"rb\") as file:\n",
    "            tfidf_matrix = pickle.load(file)\n",
    "            \n",
    "        with open(vectorizer, \"rb\") as file:\n",
    "            vectorizer = pickle.load(file)\n",
    "            \n",
    "        data = load_dataset2(data)\n",
    "            \n",
    "        query_vector = vectorizer.transform([query])\n",
    "        cosine_similarities = cosine_similarity(tfidf_matrix, query_vector).flatten()\n",
    "        n = 10\n",
    "        top_documents_indices = cosine_similarities.argsort()[-n:][::-1]\n",
    "        top_documents = data.iloc[top_documents_indices]\n",
    "    \n",
    "        return top_documents, cosine_similarities[top_documents_indices]\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluation(queries_file, tfidf_matrix_file, vectorizer_file, data_file):\n",
    "            queries = Evaluation.load_queries([queries_file])\n",
    "            all_precisions = []\n",
    "            all_recalls = []\n",
    "            all_map_scores = []\n",
    "            all_rr = []\n",
    "            \n",
    "            for query in queries:\n",
    "                if 'query' in query:\n",
    "                    processed_query = Evaluation.process_texts([query['query']])[0]\n",
    "                    print(processed_query)\n",
    "                    processed_query = processed_query['processed_text']\n",
    "                    top_documents, cosine_similarities = get_documents_for_query(processed_query, tfidf_matrix_file, vectorizer_file, data_file)\n",
    "                    \n",
    "                    data = load_dataset(data_file)\n",
    "                    relevance = np.zeros(len(data))\n",
    "                    for pid in query.get('answer_pids', []):\n",
    "                        relevance[np.where(data['pid'] == pid)[0]] = 1\n",
    "\n",
    "                    y_true = relevance[top_documents.index]\n",
    "                    y_pred = cosine_similarities\n",
    "\n",
    "                    if y_true.sum() == 0:\n",
    "                        print(f\"No relevant documents for query ID: {query.get('qid', 'N/A')}\")\n",
    "                        # print(\"Documents found:\", top_documents)\n",
    "                        continue\n",
    "\n",
    "                    precision, recall = Evaluation.calculate_precision_recall(y_true, y_pred)\n",
    "                    all_precisions.append(precision)\n",
    "                    all_recalls.append(recall)\n",
    "\n",
    "                    map_score = Evaluation.calculate_map_score(y_true, y_pred)\n",
    "                    all_map_scores.append(map_score)\n",
    "                    # Calculate reciprocal rank\n",
    "                    rr = 0\n",
    "                    for i, rel in enumerate(y_true):\n",
    "                        if rel == 1:\n",
    "                            rr = 1 / (i + 1)  # Reciprocal rank\n",
    "                            break\n",
    "                    all_rr.append(rr)\n",
    "                    print(f\"Query ID: {query.get('qid', 'N/A')}, Precision: {precision}, Recall: {recall}, MAP Score: {map_score}, RR: {rr}\")\n",
    "                    # print(\n",
    "                    #     f\"Query ID: {query.get('qid', 'N/A')}, Precision: {precision}, Recall: {recall}, MAP Score: {map_score}, RR: {rr}\")\n",
    "\n",
    "        \n",
    "            avg_precision = np.mean(all_precisions)\n",
    "            avg_recall = np.mean(all_recalls)\n",
    "            avg_map_score = np.mean(all_map_scores)\n",
    "            mrr = np.mean(all_rr)\n",
    "            # print(f\"Average Precision: {avg_precision}, Average Recall: {avg_recall}, Average MAP Score: {avg_map_score}, MRR: {mrr}\")\n",
    "            # return avg_precision ,avg_recall ,avg_map_score\n",
    "            print(f\"Average Precision: {avg_precision}, Average Recall: {avg_recall}, Average MAP Score: {avg_map_score}, MRR: {mrr}\")       \n",
    "\n",
    "    @staticmethod\n",
    "    def evaluation_clic(queries_file, tfidf_matrix_file, vectorizer_file, data_file):\n",
    "            print(\"111111111111111\")\n",
    "            queries = read_csv_to_array(queries_file)\n",
    "            all_precisions = []\n",
    "            all_recalls = []\n",
    "            all_map_scores = []\n",
    "            all_rr = []\n",
    "            print(\"22222222222\")\n",
    "            for query in queries:\n",
    "                query = [str(item) for item in query]\n",
    "                combined_query = ' '.join(query)\n",
    "                id=query[0]  \n",
    "                print(\"333333\")\n",
    "                top_documents, cosine_similarities = Evaluation.get_documents_for_query_cli(combined_query, tfidf_matrix_file,vectorizer_file, data_file)\n",
    "                qrles=r\"D:\\qrels.csv\"\n",
    "                df = pd.read_csv(qrles)\n",
    "                doc_ids , relevence = extract_doc_ids(id ,df)\n",
    "                modfied_data=r\"D:\\modified_dataset.csv\"\n",
    "                data = pd.read_csv(modfied_data)\n",
    "                relevance = np.zeros(len(data))\n",
    "                for doc_id in doc_ids:\n",
    "                        relevance[np.where(data['doc_id'] == doc_id)[0]] = 1\n",
    "\n",
    "                y_true = relevance[top_documents.index]\n",
    "                y_pred = cosine_similarities\n",
    "\n",
    "                if y_true.sum() == 0:\n",
    "                        print(f\"No relevant documents for query ID: {query[0] }\")\n",
    "                        continue\n",
    "\n",
    "                precision, recall =Evaluation.calculate_precision_recall(y_true, y_pred)\n",
    "                all_precisions.append(precision)\n",
    "                all_recalls.append(recall)\n",
    "\n",
    "                map_score = Evaluation.calculate_map_score(y_true, y_pred)\n",
    "                all_map_scores.append(map_score)\n",
    "                rr = 0\n",
    "                for i, rel in enumerate(y_true):\n",
    "                    if rel == 1:\n",
    "                        rr = 1 / (i + 1)  # Reciprocal rank\n",
    "                        break\n",
    "                all_rr.append(rr)\n",
    "\n",
    "                print(f\"Query ID: {query[0] }, Precision: {precision}, Recall: {recall}, MAP Score: {map_score}, RR: {rr}\")\n",
    "\n",
    "            avg_precision = np.mean(all_precisions)\n",
    "            avg_recall = np.mean(all_recalls)\n",
    "            avg_map_score = np.mean(all_map_scores)\n",
    "            mrr = np.mean(all_rr)\n",
    "\n",
    "            print(f\"Average Precision: {avg_precision}, Average Recall: {avg_recall}, Average MAP Score: {avg_map_score}, MRR: {mrr}\")\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "evaluation =Evaluation()\n",
    "\n",
    "yourdataset=r\"D:\\your_dataset.csv\"\n",
    "\n",
    "\n",
    "process_queries=r\"D:\\processed_queries.csv\"\n",
    "\n",
    "\n",
    "data_vector_file_cli= r\"D:\\DocumentVector_clinical.pkl\"\n",
    "\n",
    "vectorizer_file_cli= r\"D:\\TfidfVector_clinical.pkl\"  \n",
    "\n",
    "Evaluation.evaluation_clic(process_queries,data_vector_file_cli,vectorizer_file_cli,yourdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203de87e-6674-47a4-ba48-38772c427686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
